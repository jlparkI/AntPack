"""Wraps tools for constructing a searchable database from
various input types."""
import os
import math
import gc
import gzip
from ..numbering_tools import SingleChainAnnotator, PairedChainAnnotator
from ..clustering_tools import EMCategoricalMixture
from ..vj_tools.vj_gene_assignment import VJGeneTool
from ..utilities import read_fasta
from antpack.antpack_cpp_ext import (DatabaseConstructionTool,
        SequenceTemplateAligner, return_imgt_canonical_numbering_cpp)



def build_database_from_fasta(fasta_files:list,
        database_filepath:str, temp_storage_dir:str,
        numbering_scheme:str="imgt",
        cdr_definition_scheme:str="imgt",
        sequence_type:str="single", receptor_type:str="mab",
        pid_threshold:float=0.7, user_memo:str="",
        verbose:bool=True, gzip_temp_files:bool=False,
        max_threads=2):
    """Builds a database from a list of fasta file(s) which may or may
    not be gzipped. The database is constructed so it can be
    searched in sublinear time and the sequence descriptions
    for each sequence in the fasta file are saved as metadata.

    Args:
        fasta_files (str): The location of the fasta file.
        database_filepath (str): The desired location and filename
            for the database.
        temp_storage_dir (str): A location where the program can
            save a temporary file that will be deleted at the end
            of database construction.
        numbering_scheme (str): One of 'imgt', 'kabat', 'martin' or
            'aho'. If receptor_type is 'tcr', 'imgt' is the only
            allowed option.
        cdr_definition_scheme (str): One of 'imgt', 'kabat', 'martin',
            'aho' or 'north'. If receptor_type is 'tcr', 'imgt' only is allowed.
        sequence_type (str): One of 'single', 'paired', 'unknown'. If
            'paired' each sequence is assumed to be paired. If 'unknown'
            it is assumed each sequence MAY be paired and should be analyzed
            as paired just in case.
        receptor_type (str): One of 'mab', 'tcr'.
        pid_threshold (float): A value between 0 and 1 for percent identity
            threshold. If sequence_type is 'single' or 'unknown', sequences
            not meeting this threshold are excluded. If sequence_type is
            'paired' the sequences are still retained as long as one of
            the chains meets this threshold.
        user_memo (str): A string describing the purpose of the database / anything
            important you want your future self or other users to know about the
            contents. Will be saved as part of the database metadata.
        verbose (bool): If True, print regular updates while running.
        gzip_temp_files (bool): If True, temporary files generated by this algorithm
             are gzipped. This will slow construction somewhat but
             will also greatly reduce the maximum disk space footprint
             reached during construction. Set to True if maximum disk space
             footprint during construction is a concern, False if speed of
             database construction is the top priority.
        max_threads (int): The number of threads to use when clustering the
            CDR regions. Increasing this number will increase speed (but
            also usage of your computer).

    Raises:
        RuntimeError: A RuntimeError is raised if invalid arguments are
            supplied.
    """
    if os.path.exists(database_filepath):
        raise RuntimeError("The database already exists.")

    # First, construct a temporary file containing the numbered and aligned
    # sequences.
    if receptor_type == "mab":
        chains = ["H", "K", "L"]
    else:
        chains = ["A", "B", "D", "G"]

    vj_tool = VJGeneTool(scheme=numbering_scheme)

    if numbering_scheme == "imgt":
        heavy_canon_nmbr = return_imgt_canonical_numbering_cpp()
        light_canon_nmbr = return_imgt_canonical_numbering_cpp()
    else:
        raise RuntimeError("An invalid numbering scheme was "
                "supplied for the receptor type you supplied.")

    heavy_aligner = SequenceTemplateAligner(heavy_canon_nmbr,
            chains[0], "imgt", cdr_definition_scheme)
    light_aligner = SequenceTemplateAligner(light_canon_nmbr,
            chains[2], "imgt", cdr_definition_scheme)

    if verbose:
        print("Now constructing a temporary file containing numbered & "
                "aligned sequences.")

    if gzip_temp_files:
        temp_fname = os.path.join(temp_storage_dir,
                "TEMPORARY_FILE.txt.gz")
        storage_handle = gzip.open(temp_fname, "wt")
    else:
        temp_fname = os.path.join(temp_storage_dir,
                "TEMPORARY_FILE.txt")
        storage_handle = open(temp_fname, "w+", encoding="utf-8")

    chain_counts = [0,0]

    if sequence_type == "single":
        sca_tool = SingleChainAnnotator(chains=chains, scheme=numbering_scheme)

        for fasta_file in fasta_files:
            for i, (seqinfo, seq) in enumerate(read_fasta(fasta_file)):
                if i % 10000 == 0 and verbose:
                    print(f"    {i} complete.")
                try:
                    annotation = sca_tool.analyze_seq(seq)
                    if annotation[1] < pid_threshold:
                        continue
                    heavy_vgenes, heavy_jgenes, light_vgenes, light_jgenes = \
                            "", "", "", ""
                    heavy_species, light_species = "", ""
                    aligned_heavy, aligned_light = "", ""

                    if annotation[2] in ("A", "G", "H"):
                        chain_counts[0] += 1
                        aligned_heavy = heavy_aligner.align_sequence(
                                seq, annotation[0])
                        heavy_vgenes, heavy_jgenes, _, _, heavy_species = \
                                vj_tool.assign_vj_genes(
                            annotation, seq, "unknown", "identity")
                    else:
                        chain_counts[1] += 1
                        aligned_light = light_aligner.align_sequence(seq,
                                annotation[0])
                        light_vgenes, light_jgenes, _, _, light_species = \
                                vj_tool.assign_vj_genes(
                            annotation, seq, "unknown", "identity")
                except Exception as e:
                    print(e)
                    storage_handle.close()
                    os.remove(temp_fname)

                storage_handle.write(f"{seq},{aligned_heavy},{aligned_light},"
                        f"{heavy_vgenes},{heavy_jgenes},{heavy_species},"
                        f"{light_vgenes},{light_jgenes},{light_species},"
                        f"{seqinfo},,\n")

        del sca_tool

    else:
        pca_tool = PairedChainAnnotator(numbering_scheme, receptor_type)

        for fasta_file in fasta_files:
            for i, (seqinfo, seq) in enumerate(read_fasta(fasta_file)):
                if i % 10000 == 0 and verbose:
                    print(f"    {i} complete.")
                try:
                    heavy_annotation, light_annotation = pca_tool.analyze_seq(seq)
                    if heavy_annotation[1] < pid_threshold and \
                            light_annotation[1] < pid_threshold:
                        continue
                    heavy_vgenes, heavy_jgenes, light_vgenes, light_jgenes = \
                            "", "", "", ""
                    heavy_species, light_species = "", ""
                    aligned_heavy, aligned_light = "", ""

                    if heavy_annotation[1] >= pid_threshold:
                        chain_counts[0] += 1
                        aligned_heavy = heavy_aligner.align_sequence(
                                seq, heavy_annotation[0])
                        heavy_vgenes, heavy_jgenes, _, _, heavy_species = \
                                vj_tool.assign_vj_genes(
                            heavy_annotation, seq, "unknown", "identity")
                    if light_annotation[1] >= pid_threshold:
                        chain_counts[1] += 1
                        aligned_light = light_aligner.align_sequence(seq,
                                light_annotation[0])
                        light_vgenes, light_jgenes, _, _, light_species = \
                                vj_tool.assign_vj_genes(
                            light_annotation, seq, "unknown", "identity")
                except Exception as e:
                    print(e)
                    storage_handle.close()
                    os.remove(temp_fname)

                storage_handle.write(f"{seq},{aligned_heavy},{aligned_light},"
                        f"{heavy_vgenes},{heavy_jgenes},{heavy_species},"
                        f"{light_vgenes},{light_jgenes},{light_species},"
                        f"{seqinfo},\n")

        del pca_tool

    storage_handle.close()
    del vj_tool, heavy_aligner, light_aligner
    # Not really necessary because the memory footprint of this should overall
    # be quite small, but nonetheless good manners.
    gc.collect()

    if verbose:
        print("*****\nSequence preprocessing complete. Now beginning CDR clustering. ")

    cluster_models = _cluster_cdr_regions(temp_fname, chain_counts, [heavy_canon_nmbr,
        light_canon_nmbr], temp_storage_dir, verbose,
        numbering_scheme, cdr_definition_scheme, max_threads,
        receptor_type)
    if verbose:
        print("*****\nClustering complete. Now constructing database...")

    db_construct_tool = DatabaseConstructionTool(database_filepath,
            numbering_scheme, cdr_definition_scheme,
            sequence_type, receptor_type,
            pid_threshold, user_memo)

    db_construct_tool.open_transaction()

    # Process lines from the storage file in batches for more efficiency
    # in cluster assignment / profile update.
    line_batch = []

    for i, line in enumerate(storage_handle):
        if i % 10000 == 0:
            db_construct_tool.close_transaction()
            db_construct_tool.open_transaction()
            if verbose:
                print(f"{i} complete.")
        line_batch.append(line)
        if len(line_batch) > 100:
            _prep_line_batch(cluster_models, line_batch,
                    db_construct_tool)

    if len(line_batch) > 0:
        _prep_line_batch(cluster_models, line_batch,
                db_construct_tool)


    if verbose:
        print("Now constructing database indices...")

    db_construct_tool.close_transaction()
    db_construct_tool.open_transaction()
    db_construct_tool.finalize_db_construction()
    db_construct_tool.close_transaction()

    if verbose:
        print("Database construction complete. Removing temporary storage file.")
    os.remove(temp_fname)




def _cluster_cdr_regions(storage_fname:str, chain_counts:list,
        chain_canon_nmbr:list, temp_storage_dir:str,
        verbose:bool, numbering_scheme:str, cdr_scheme:str,
        max_threads:int, receptor_type:str="mab"):
    """Clusters the cdr regions using as input a temporary file
    containing prealigned sequences and other useful information."""
    cluster_models = []

    if receptor_type == "mab":
        chain_codes = ["H", "L"]
    else:
        chain_codes = ["A", "B"]

    for (chain, chain_designator) in [("heavy", 0), ("light", 1)]:
        for region in ["cdr1", "cdr2", "cdr3"]:
            if verbose:
                print(f"Now clustering chain {chain}, region {region}")

            if chain_counts[chain_designator] == 0:
                cluster_models.append(None)
                continue

            # This is a heuristic. May find a better way to set this
            # in future.
            nclusters = int(10 * max(1,
                    math.log10(chain_counts[chain_designator] / 100)))
            em_cluster = EMCategoricalMixture(n_components=nclusters,
                    numbering=chain_canon_nmbr[chain_designator],
                    chain_type=chain_codes[chain_designator],
                    numbering_scheme=numbering_scheme,
                    cdr_scheme=cdr_scheme,
                    region=region, max_threads=max_threads,
                    verbose=verbose)
            file_list = em_cluster.encode_temp_db_prep_file(storage_fname,
                    temp_storage_dir, chain_designator + 1,
                    chunk_size=5000)
            em_cluster.fit(filepaths=file_list, max_iter=25,
                    tol=1e-2, n_restarts=3, random_state=123,
                    prune_after_fitting=True)

            cluster_models.append(em_cluster)

            for fpath in file_list:
                os.remove(fpath)

    return cluster_models


def _prep_line_batch(cluster_models, line_batch, db_construct_tool):
    """Adds a batch of lines loaded from a temp storage file
    to the database. Used internally by AntPack only."""
    line_elements = [l.split(',') for l in line_batch]
    heavy_seqs = [l[1] for l in line_elements]
    light_seqs = [l[2] for l in line_elements]

    
